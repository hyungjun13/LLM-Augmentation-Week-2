{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll build a basic LangChain agent. We'll leverage OpenAI for LLM responses and implement various tools to create a flexible, dynamic workflow. LangChain's agent framework will allow us to orchestrate different functions and API calls seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "import os\n",
    "from rag import RagEngine \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0,\n",
    "    model_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Decorator Approach\n",
    "\n",
    "The `@tool` decorator in LangChain is used to easily register functions as tools that agents can use. It simplifies integration by marking the function for use in LangChain workflows without needing extra configuration.\n",
    "\n",
    "In this case, the `@tool` decorator:\n",
    "- Registers the `calculator` function as a callable tool for agents.\n",
    "- Allows the agent to evaluate mathematical expressions safely by using `eval()` with restricted input handling.\n",
    "\n",
    "This approach makes it easy to add modular, reusable tools to LangChain agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(input_expr: str) -> int:\n",
    "    \"\"\"A simple calculator tool that evaluates a mathemtical expression and returns the result as an integer. Do not pass untrusted input.\"\"\"\n",
    "    return eval(input_expr.replace(\"^\", \"**\"), {}, {})\n",
    "\n",
    "tools = [calculator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3\n",
    ")\n",
    "\n",
    "zero_shot_agent(\"what is (4.5*2.1)^2.2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Tools for the Agent\n",
    "\n",
    "In this section, we define two tools that the agent will use:\n",
    "\n",
    "1. **QA Chat Tool**: A simple question-answering tool that takes a user message and generates a response using a large language model (LLM).\n",
    "2. **RAG Chat Tool**: A more complex tool that handles document retrieval using Retrieval-Augmented Generation (RAG). This tool is specifically designed to retrieve and generate responses from environmental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the QA Chat Tool\n",
    "# This tool handles normal question-answering interactions by invoking the LLM\n",
    "@tool\n",
    "def qa_chat_tool(message: str) -> str:\n",
    "    \"\"\"Tool for handling normal question-answer (QA) interactions.\"\"\"\n",
    "\n",
    "    template = \"\"\"Respond to the user message:\n",
    "        message: {message}\n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke(message)\n",
    "    return response # Return the generated response\n",
    "\n",
    "# Define the RAG Chat Tool: Feel free to change the rag document and description to add a different document\n",
    "# This tool handles document retrieval and generation using RAG from environmental data\n",
    "@tool\n",
    "def rag_chat_tool(message: str) -> str:\n",
    "    \"\"\"Tool for handling document retrieval and generating responses using RAG from environmental data.\"\"\"\n",
    "    \n",
    "    # Initialize the Retrieval-Augmented Generation (RAG) engine\n",
    "    rag_engine = RagEngine()\n",
    "    \n",
    "    # Use the RAG engine to return an answer to the user's query\n",
    "    response = rag_engine.return_answer(message)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling Application with Tools in LangChain\n",
    "\n",
    "In this example, we are creating a function-calling agent using LangChain. The agent is set up with tools such as `qa_chat_tool` and `rag_chat_tool`, which can be used for question-answering and retrieval-augmented generation tasks. The agent is initialized as a zero-shot agent, meaning it can take actions based on descriptions of the tools provided to it without requiring training on specific tasks.\n",
    "\n",
    "The `max_iterations` parameter limits the number of decision-making steps the agent can take before completing a task. Here's how to set up and initialize the agent with tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools the agent will use\n",
    "tools = [qa_chat_tool, rag_chat_tool]\n",
    "\n",
    "# Initialize the zero-shot agent\n",
    "# The agent is based on the \"zero-shot-react-description\" model, meaning it uses \n",
    "# reasoning and action descriptions to perform tasks without explicit training\n",
    "zero_shot_agent = initialize_agent(\n",
    "    agent=\"zero-shot-react-description\",  # Agent type for reasoning and decision making\n",
    "    tools=tools,  # List of tools the agent will have access to\n",
    "    llm=llm,  # The large language model (LLM) that powers the agent\n",
    "    verbose=True,  # Set to True to see detailed output of the agent's decision-making process\n",
    "    max_iterations=3  # Limit the agent to 3 iterations when performing actions\n",
    ")\n",
    "\n",
    "# You can now use zero_shot_agent to run function calls based on input queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one may take a while oops\n",
    "zero_shot_agent(\"what is the cause of global warming?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_agent(\"What type of food is an apple?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Your Turn\n",
    "\n",
    "Now, create a function-calling application of your own. You are encouraged to use external APIs to create robust tools. Feel free to change the rag engine to a more personalized document. You will may need to change rag.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def MyTool(___):\n",
    "    \"A description of the tool\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
